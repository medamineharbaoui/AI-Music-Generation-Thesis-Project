# Application of artificial intelligence methods in music generation: Classical Music Generation

## Overview
**My Thesis Project** is a classical music generation framework that uses advanced neural network architectures to compose original MIDI music. It leverages **LSTM**, **Transformer**, and **Hybrid LSTM-Transformer** models to generate music, offering a flexible platform for developers, musicians, and AI enthusiasts to train and experiment with music generation.

This project provides scripts and notebooks to train models, pre-trained weights for immediate music generation, and a structured approach to generate MIDI files using different architectures.

## Project Structure
The project is organized as follows:

```
├── input/                    # Input dataset (MIDI files)
├── notebooks/                # Notebooks for training
│   ├── train_lstm.ipynb   # Notebook for LSTM model training
│   ├── train_transformer.ipynb # Notebook for Transformer model training
│   ├── train_hybrid.ipynb # Notebook for Hybrid LSTM-Transformer training
├── thesis_models/            # Pre-trained models and weights
│   ├── LSTM/                 # LSTM model and best weights
│   ├── Transformer/          # Transformer model and best weights
│   ├── Hybrid/               # Hybrid model and best weights
├── thesis_generated_music/   # Generated MIDI files
│   ├── LSTM/                 # Music generated by LSTM model
│   ├── Transformer/          # Music generated by Transformer model
│   ├── Hybrid/               # Music generated by Hybrid model
├── lstm_predict.py           # Script to generate music using LSTM model
├── transformer_predict.py    # Script to generate music using Transformer model
├── hybrid_predict.py         # Script to generate music using Hybrid model
├── train_lstm.py             # Script to train LSTM model locally
├── train_transformer.py      # Script to train Transformer model locally
└── README.md                 # This file
```

## Downloads

Download missing thesis_models content and thesis_generated_music content from this link :

```bash

   https://drive.google.com/drive/folders/1UOgHilxtPSadcc3Uu8_NKs7hN65Du-Nu?usp=sharing

   ```

## Approach
The project follows a hybrid development and training workflow to optimize resource usage:

1. **Local Development**:
   - Training scripts (`train_lstm.py`, `train_transformer.py`) are written and tested locally using a small dataset to ensure functionality and debug issues.
   - This minimizes the use of limited cloud resources during the debugging phase.

2. **Cloud Training**:
   - Training is performed on Kaggle using Jupyter notebooks (`lstm_training.ipynb`, `transformer_training.ipynb`, `hybrid_training.ipynb`) to leverage the **GPU P100 accelerator**.
   - The dataset is uploaded to Kaggle, and models are trained within Kaggle’s 12-hour session limit and 30-hour weekly GPU quota.
   - After training, the best model weights are downloaded to the local project under `thesis_models/`.

3. **Local Prediction**:
   - Pre-trained models and weights in `thesis_models/` are used with prediction scripts (`lstm_predict.py`, `transformer_predict.py`, `hybrid_predict.py`) to generate MIDI music locally.
   - Generated music is saved in `thesis_generated_music/` under respective model folders.

## Getting Started

### Prerequisites
- Python 3.8+
- Required libraries: `tensorflow`, `numpy`, `music21`, `midi2audio` (install via `pip install -r requirements.txt` if provided, or install individually)
- Kaggle account (for training on Kaggle with GPU)
- MIDI player or DAW (e.g., MuseScore, Ableton Live) to listen to generated MIDI files

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/medamineharbaoui/AI-Music-Generation-Thesis-Project.git
   cd AI-Music-Generation-Thesis-Project
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Training Models
#### Option 1: Local Training
- Use a small dataset in `input/` for testing.
- Run training scripts:
  ```bash
  python train_lstm.py
  python train_transformer.py
  ```
- Note: Local training is recommended only for debugging due to computational requirements.

#### Option 2: Kaggle Training (Recommended)
1. Upload the dataset to a Kaggle kernel.
2. Import the notebooks (`lstm_training.ipynb`, `transformer_training.ipynb`, `hybrid_training.ipynb`) to Kaggle.
3. Run the notebooks with the **GPU P100 accelerator** enabled.
4. Download the trained model weights and save them in `thesis_models/`.

### Generating Music
1. Ensure pre-trained weights are in `thesis_models/`.
2. Run the prediction scripts:
   ```bash
   python lstm_predict.py
   python transformer_predict.py
   python hybrid_predict.py
   ```
3. Generated MIDI files will be saved in `thesis_generated_music/`.

### Listening to Generated Music
- Open MIDI files in `thesis_generated_music/` using a MIDI player or DAW.
- Optionally, convert MIDI to audio using `midi2audio` or a DAW.

## Notes
- Training on Kaggle is recommended to leverage GPU acceleration and avoid local resource constraints.
- Ensure the dataset in `input/` is properly formatted (MIDI files) before training.
- Prediction scripts assume pre-trained weights are available in `thesis_models/`.



