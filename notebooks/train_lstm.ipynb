{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821af48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install music21\n",
    "\n",
    "# Import libraries\n",
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, BatchNormalization as BatchNorm\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"/kaggle/working/lstm_outputs/model_notes\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/lstm_outputs/weights\", exist_ok=True)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "def train_network():\n",
    "    notes = get_notes()\n",
    "    n_vocab = len(set(notes))\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "    model = create_network(network_input, n_vocab)\n",
    "    train_model(model, network_input, network_output)\n",
    "\n",
    "def get_notes():\n",
    "    notes = []\n",
    "    files = glob.glob(\"/kaggle/input/d/medamineharbaoui/midi-songs/midi_songs/*.mid\")[:100]\n",
    "    for file in files:\n",
    "    #for file in glob.glob(\"/kaggle/input/d/medamineharbaoui/midi-songs/midi_songs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "        print(f\"Parsing {file}\")\n",
    "        notes_to_parse = None\n",
    "        try:\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse()\n",
    "        except:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "        for element in notes_to_parse:\n",
    "            duration = element.duration.quarterLength\n",
    "            if duration < 0.75:\n",
    "                duration_class = 'short'\n",
    "            elif duration < 1.5:\n",
    "                duration_class = 'medium'\n",
    "            else:\n",
    "                duration_class = 'long'\n",
    "            if isinstance(element, note.Note):\n",
    "                note_str = f\"{str(element.pitch)}_{duration_class}\"\n",
    "                notes.append(note_str)\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                chord_str = f\"{'.'.join(str(n) for n in element.normalOrder)}_{duration_class}\"\n",
    "                notes.append(chord_str)\n",
    "    with open('/kaggle/working/lstm_outputs/model_notes/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    return notes\n",
    "\n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    sequence_length = 25\n",
    "    note_names = sorted(set(notes))\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(note_names))\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    for i in range(len(notes) - sequence_length):\n",
    "        input_sequence = notes[i:i + sequence_length]\n",
    "        output_sequence = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in input_sequence])\n",
    "        network_output.append(note_to_int[output_sequence])\n",
    "    n_patterns = len(network_input)\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    network_output = to_categorical(network_output)\n",
    "    return network_input, network_output\n",
    "\n",
    "def create_network(network_input, n_vocab):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), recurrent_dropout=0.3, return_sequences=True))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return model\n",
    "\n",
    "def train_model(model, network_input, network_output):\n",
    "    filepath = \"/kaggle/working/lstm_outputs/weights/weights-improve-LSTM-{epoch:02d}-{loss:.4f}.keras\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    model.fit(network_input, network_output, epochs=200, batch_size=32, callbacks=callbacks_list, verbose=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0122ac3",
   "metadata": {},
   "source": [
    "check best loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796d585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Directory containing .keras files\n",
    "weights_dir = \"/kaggle/working/lstm_outputs/weights/\"\n",
    "\n",
    "# Function to extract epoch and loss from filename\n",
    "def parse_filename(filename):\n",
    "    match = re.search(r\"weights-improve-LSTM-(\\d+)-(\\d+\\.\\d{4})\\.keras\", filename)\n",
    "    if match:\n",
    "        epoch = int(match.group(1))\n",
    "        loss = float(match.group(2))\n",
    "        return epoch, loss\n",
    "    return -1, float(\"inf\")\n",
    "\n",
    "# Get all .keras files and extract epoch/loss\n",
    "keras_files = glob.glob(weights_dir + \"*.keras\")\n",
    "files_with_loss = [(f, parse_filename(f)) for f in keras_files]\n",
    "files_with_loss = [(f, epoch, loss) for f, (epoch, loss) in files_with_loss if epoch != -1]\n",
    "\n",
    "# Find the file with the lowest loss\n",
    "if files_with_loss:\n",
    "    best_file, best_epoch, best_loss = min(files_with_loss, key=lambda x: x[2])\n",
    "    print(f\"Best file: {best_file}\")\n",
    "    print(f\"Epoch: {best_epoch}, Loss: {best_loss}\")\n",
    "\n",
    "    # Create a zip file with the best file\n",
    "    zip_path = \"/kaggle/working/best_keras_weight.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(best_file, os.path.join(\"weights\", os.path.basename(best_file)))\n",
    "    print(f\"Zip file created at: {zip_path}\")\n",
    "else:\n",
    "    print(\"No valid .keras files found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
